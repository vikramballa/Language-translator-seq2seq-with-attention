{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2276/2276 [03:45<00:00, 10.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2276/2276 [03:50<00:00,  9.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2276/2276 [03:50<00:00,  9.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2276/2276 [03:49<00:00,  9.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2276/2276 [03:48<00:00,  9.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2276/2276 [03:49<00:00,  9.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2276/2276 [03:50<00:00,  9.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2276/2276 [03:49<00:00,  9.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2276/2276 [03:49<00:00,  9.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2276/2276 [03:50<00:00,  9.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2276/2276 [03:53<00:00,  9.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2276/2276 [03:51<00:00,  9.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2276/2276 [04:10<00:00,  9.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2276/2276 [03:59<00:00,  9.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2276/2276 [04:01<00:00,  9.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2276/2276 [03:58<00:00,  9.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2276/2276 [03:58<00:00,  9.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2276/2276 [03:55<00:00,  9.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2276/2276 [03:59<00:00,  9.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2276/2276 [03:57<00:00,  9.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20, Loss: nan\n",
      "Hindi Translation: आरंभ करें (_I) gedit.gnome-2-2.hi.po #-#-#-#-# #-#-#-#-# #-#-#-#-# #-#-#-#-# #-#-#-#-# #-#-#-#-# #-#-#-#-# #-#-#-#-# #-#-#-#-# #-#-#-#-# #-#-#-#-# #-#-#-#-# #-#-#-#-# #-#-#-#-# #-#-#-#-# #-#-#-#-#\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "latent_dim = 128\n",
    "embedding_dim = 128\n",
    "max_encoder_seq_length = 20\n",
    "max_decoder_seq_length = 20\n",
    "vocab_size_limit = 20000\n",
    "sample_size = int(0.8 * 900000)\n",
    "batch_size = 64\n",
    "epochs = 20\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def load_data(english_file, hindi_file, sample_size):\n",
    "    with open(english_file, 'r', encoding='utf-8') as f_en, open(hindi_file, 'r', encoding='utf-8') as f_hi:\n",
    "        english_texts = f_en.read().splitlines()[:sample_size]\n",
    "        hindi_texts = f_hi.read().splitlines()[:sample_size]\n",
    "    return english_texts, hindi_texts\n",
    "\n",
    "english_texts, hindi_texts = load_data(\"GNOME.en-hi.en\", \"GNOME.en-hi.hi\", sample_size)\n",
    "\n",
    "\n",
    "def build_tokenizer(texts, vocab_size_limit):\n",
    "    token_counts = Counter(word for text in texts for word in text.split())\n",
    "    most_common_tokens = token_counts.most_common(vocab_size_limit - 2)\n",
    "    word_to_index = {word: i + 2 for i, (word, _) in enumerate(most_common_tokens)}\n",
    "    word_to_index[\"<PAD>\"] = 0\n",
    "    word_to_index[\"<START>\"] = 1\n",
    "    index_to_word = {i: word for word, i in word_to_index.items()}\n",
    "    return word_to_index, index_to_word\n",
    "\n",
    "input_word_to_index, input_index_to_word = build_tokenizer(english_texts, vocab_size_limit)\n",
    "target_word_to_index, target_index_to_word = build_tokenizer(hindi_texts, vocab_size_limit)\n",
    "\n",
    "\n",
    "def texts_to_sequences(texts, word_to_index, max_length):\n",
    "    sequences = []\n",
    "    for text in texts:\n",
    "        seq = [word_to_index.get(word, word_to_index[\"<PAD>\"]) for word in text.split()]\n",
    "        sequences.append(seq[:max_length] + [word_to_index[\"<PAD>\"]] * (max_length - len(seq)))\n",
    "    return np.array(sequences)\n",
    "\n",
    "input_sequences = texts_to_sequences(english_texts, input_word_to_index, max_encoder_seq_length)\n",
    "target_sequences = texts_to_sequences(hindi_texts, target_word_to_index, max_decoder_seq_length)\n",
    "\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, encoder_data, decoder_data):\n",
    "        self.encoder_data = encoder_data\n",
    "        self.decoder_data = decoder_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoder_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoder_input = torch.tensor(self.encoder_data[idx], dtype=torch.long)\n",
    "        decoder_input = torch.tensor(self.decoder_data[idx], dtype=torch.long)\n",
    "        return encoder_input, decoder_input\n",
    "\n",
    "dataset = TranslationDataset(input_sequences, target_sequences)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(latent_dim * 2, max_encoder_seq_length)\n",
    "        self.v = nn.Parameter(torch.rand(max_encoder_seq_length))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, max_encoder_seq_length, 1)\n",
    "        attn_energies = torch.sum(self.v * torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), 2))), dim=2)\n",
    "        return torch.softmax(attn_energies, dim=1).unsqueeze(1)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, latent_dim, batch_first=True)\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, latent_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim + latent_dim, latent_dim, batch_first=True)\n",
    "        self.attention = Attention(latent_dim)\n",
    "        self.fc = nn.Linear(latent_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, input, hidden, cell, encoder_outputs):\n",
    "        input = input.unsqueeze(1)\n",
    "        embedded = self.embedding(input)\n",
    "        attn_weights = self.attention(hidden[-1], encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs)\n",
    "        lstm_input = torch.cat((embedded, context), dim=2)\n",
    "        output, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n",
    "        prediction = self.fc(torch.cat((output.squeeze(1), context.squeeze(1)), dim=1))\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "\n",
    "encoder = Encoder(len(input_word_to_index), embedding_dim, latent_dim).to(device)\n",
    "decoder = Decoder(len(target_word_to_index), embedding_dim, latent_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.001)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for encoder_input, decoder_target in tqdm(data_loader):\n",
    "        encoder_input, decoder_target = encoder_input.to(device), decoder_target.to(device)\n",
    "        encoder_outputs, hidden, cell = encoder(encoder_input)\n",
    "        decoder_input = torch.tensor([target_word_to_index[\"<START>\"]] * batch_size).to(device)\n",
    "\n",
    "        loss = 0\n",
    "        use_teacher_forcing = np.random.rand() < (1 - epoch / epochs)\n",
    "        for t in range(1, decoder_target.size(1)):\n",
    "            output, hidden, cell = decoder(decoder_input, hidden, cell, encoder_outputs)\n",
    "            loss += criterion(output, decoder_target[:, t])\n",
    "\n",
    "            decoder_input = decoder_target[:, t] if use_teacher_forcing else output.argmax(1)\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "        total_loss += loss.item() / decoder_target.size(1)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(data_loader):.4f}\")\n",
    "\n",
    "\n",
    "def translate(input_text):\n",
    "    with torch.no_grad():\n",
    "        input_seq = texts_to_sequences([input_text], input_word_to_index, max_encoder_seq_length)\n",
    "        encoder_input = torch.tensor(input_seq).to(device)\n",
    "        \n",
    "        encoder_outputs, hidden, cell = encoder(encoder_input)\n",
    "        decoder_input = torch.tensor([target_word_to_index[\"<START>\"]]).to(device)\n",
    "\n",
    "        translated_sentence = []\n",
    "        for _ in range(max_decoder_seq_length):\n",
    "            output, hidden, cell = decoder(decoder_input, hidden, cell, encoder_outputs)\n",
    "            top1 = output.argmax(1)\n",
    "            if top1.item() == target_word_to_index[\"<PAD>\"]:\n",
    "                break\n",
    "            translated_sentence.append(target_index_to_word.get(top1.item(), \"\"))\n",
    "            decoder_input = top1\n",
    "\n",
    "    return ' '.join(translated_sentence)\n",
    "\n",
    "\n",
    "user_input = input(\"Enter a sentence in English: \")\n",
    "print(\"Hindi Translation:\", translate(user_input))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
